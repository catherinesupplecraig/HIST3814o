# Fail Log: Module 4 - 2017-08-04

#### R Studio & Topic Modeling
* Installed R and RStudio on my own machine for this exercise (DH box was still down when I started)
* I began by reading through the quick intro to R
* I then installed Swirl and went through a few of the lessons - they were all kind of mathy, which I didn't think was super helpful for my purposes, but it helped me learn the syntax
* Couldn’t for the life of me set the working directory as the folder I’d created (beals) for this exercise
* Some googling revealed that if you drag the folder into the RStudio application icon (on a mac) it will open up with this file as its directory! This is cool, but I've also now figured out how to open set the working directory from within Rstudio. Trial and error, trial and error! 
* Hit an error when I tried to execute the command 'library(rJava)’ which caused a fatal error the first time I tried to run it. Apparently RStudio requires a legacy version of java to do this? It led me to the apple support page where I could download it
* Installed mallet and Rcurl
* First major snag I hit was while trying to do the distant reading of the colonial newspaper database. Link to the stoplist we were provided was broken, so I googled A) what is a stoplist? and once I'd figured out B) Jocker's stoplist, which brought me to a page that had [Matthew Jocker's stoplist](http://www.matthewjockers.net/macroanalysisbook/expanded-stopwords-list/), which I then copied and pasted into sublime, then saved as a .txt
* As a side note, I first thought I may have pasted this weird, or accidentally deleted a word, because when I got further into the exercise the word 'the' was one of the most frequent words showing up in my results. When I actually went and looked at jocker's stop list (or the version I had, anyway), it was mostly proper names, so I would defintely go back and add some words if I were to use this stoplist again. Productive fail, I guess
* I hit a snag using this command `mallet.instances <- mallet.import(documents$Article_ID, documents$Text, “jockersstoplist.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")` and it took me AGES to resolve. I tried a bunch of different variations, according to what I'd googled and the error message, but I kept having issues - finally I got a version that worked - I think making sure there aren't any superfluous spaces might have been what did the trick, but when I tried a similar command in the Ferguson topic modeling exercise I had the same problem. I found a supposed fix on stackoverflow that I thought had worked, but it ultimately failed as well. All of my trial and error can be found in the files uploaded to my repository.
* I ended up with some pretty cool visualizations! It was interesting to see the difference between my results and the examples in the exercise - for instance, I had one result where the words 'wool' and 'trade' were very significant, whereas the corresponding example was all about citiens and states.

### iGraph
* I installed igraph in R
* I used igraph to run through a series of network visualizations based on the Republic of Texas correspondance we worked with last week.
* It was interesting to see the relationships between some of the people whose named I'd gotten familiar with through Open Refine
* Using igraph was so much fun! The network visulizations look super cool and weird. 
* I was actually able to run through this exercise without any major blunders, but as I was doing it I found myself thinking a lot about how network analysis could be used to track dimensions other than interpersonal relationships. I talked about this a little bit in my blog post for the week.

### Personal Fail
The past two weeks I've set the goal of being more engaged, and I actually feel like I've made some headway in that area this week, so that's progress. Honestly, half the battle is how hard I am on myself when I'm not living up to my own expectations. This week's personal fail is failing to take it slow. So often, when I'm going through the exercises I find myself following along with the steps, without really absorbing what I'm doing, and that's a kind of a big problem because, um, how am I supposed to do my final project if I don't actually know what I'm doing? Eeek! I think I actually know more than I think I do. I mean, it makes sense that I would read something about R or network visualization or whatever and I have no idea what it means until I'd actually tried it, or seen it put into practice. I've said before that trial and error is the way I tend to learn best, and I definitely think that the "productive fail" moments of this course are the ones that have helped me absorb what i'm learning in the most perminant way. I still do need to work on slowing down, making sure I'm reading everything, and really trying to understand. Goal for next week!  

# Fail Log: Module 3 - 2017-07-30

### Exercise 1: Regular Expressions
* My first reaction to looking through the steps of this exercise was extreme panic and fear
* To begin, I used the terminal on my computer and the `curl` command to grab the Diplomatic Correspondence of the Republic of Texas file on located on archive.org [here](http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt)
* Reminder to self: `curl` works in a similar way to `wget`
* Before working through the sed commands for this exercise, I wanted to copy text from my texas.txt nano file in the terminal to RegExr so that I could test my expressions before trying them for real, but struggled with figuring out how to copy text from nano. I looked up the command in the [how-to-geek beginner’s guide to nano](https://www.howtogeek.com/howto/42980/the-beginners-guide-to-nano-the-linux-command-line-text-editor/), got confused by their use of the term "meta key", figured out that "alt" is usually the equivalent to meta on a mac, tried the command…and it didn’t work. I did some further googling (there were some tutorials that suggested changing the keyboard settings on your mac, but they were all quite out of date) On the upside, I brushed up on some of my nano commands by reading through the guide again. Productive fail? 
* I ultimately solved the problem by saving my text as a markdown file and opening with sublime, then copying to RegExr
* I struggled with every step of this exercise, and ended up watching the helpful video posted in our group slack to figure out where I was going wrong. I was able to solve a few issues on my own through trial and error, and felt pretty accomplished when things went right…only to be frustrated a second later when the command wouldn’t work.
* I kept getting a weird error when putting in the command to replace the tilde with a space, which formatted as `sed -E -i.bak ’s/~/ /g’ index.txt` 
* The command wouldn’t run, but instead of an error I’d just get a new line with `>` at the beginning. I was pretty sure my command was formatted correctly after looking at the example on the workbook and watching the video, but eventually ended up copying from the workbook and running it, which worked. 
* When I compared the two commands they looked identical at first ([here’s a screenshot](https://github.com/catherinesupplecraig/HIST3814o/blob/master/Screen%20Shot%202017-07-30%20at%208.16.10%20PM.png)), but I then realized that instead of using an apostrope `'`, I'd been using a backtick `` `. Whoopsie!
* I saved my history for this exercise [here](https://github.com/catherinesupplecraig/HIST3814o/blob/master/july30.md), and in doing so learned that when you save history from your own terminal, it saves everything you’ve ever done - handy, but could also become kind of long-winded. In any case, you can see all my trial and error. (Note - the first two lines of my history are irrelevant to this exercise, and are on there from something run in the past) 

### Exercise 2: Open Refine
* I used [OpenRefine](http://openrefine.org/) to clean up the data in my cleanedcorrespondance.csv
* It took two attempts for me to successfully refine the table using clustering techniques - I had to start over about halfway through my first attempt because after I tried to use the "undo" function something went terribly terribly wrong.
* I was able to use clustering to sort through my "sender" and "receiver" columns until I had around the indicated number of results remaining. 
* I used the text facet to sort each column, and applied clustering techniques to refine the data. I played around with the different methods and keying functions and was able to get more data to merge. 
* I uploaded my finished product as a csv to my github repistory [here](https://github.com/catherinesupplecraig/HIST3814o/blob/master/Module3-Exercise2.csv)
* I used Connect The Dots to create a visualization of my cleaned data (when I tried to use Palladio as suggested I just got a blank page with three red dots. Not sure what that's about) and my finished product can be viewed [here](https://databasic.io/en/connectthedots/results/597e85ec7088b44d32a91f0a?submit=true)

### Fail Log
* Funny story - I normally write my fail log in dillinger.io and then merge my changes onto github, however, dillinger isn't working for me this evening (as in, I can't seem to connect to the site), meaning that I had to find another solution to write this.
* I considered downloading my faillog.md file off of github, opening in nano and doing my writing there, but one of the things I like about dillinger is that it lets me see what I'm doing on the split screen, which helps reduce any formatting errors that might come up.
* I did some googling and found StackEdit, which I'm using now. Unfortunately StackEdit doesn't allow you to import files directly from github, from what I could tell.
* My solution for this was to copy the raw version of my failog from github into the editing window on StackEdit. Once I'm done here, I'll edit my github file and paste in the raw text from StackEdit directly into my faillog.md file on github. Hopefully it works!
* Update: It didn't work. I kept getting an error fro StackEdit saying that my file couldn't be uploaded to github every time I tried... BUT dillinger.io is back online again :)

### Personal Fail
* This week's personal fail is all about motivation. Last week, I felt like I had dropped the ball on the interactive aspect of the course. I made a bold statement that I would do better this week and...I failed. I would not say that I've been more engaged this week than last week. It's so weird to me that the easiest part of the course, in terms of what I already know how to do (talk to other human beings, write, etc) has been the hardest for me so far. This week I'm going to attribute that to a dip in my motivation. I'm still really interested in what we're learning, and I'm enjoying the course as much as ever, but even though I was putting in the hours to get the course work done over the week, I felt like my productivity was lower. I didn't want to study - I wanted to go hang out with my friends and eat pizza and watch Dunkirk and go see La Machine and have drinks on a patio... and that's what I did instead of doing more annotations and just generally getting more engaged. Summer courses can be tough like that. I will say though, staying motivated in this course has been easier than in a lot of other classes I've taken in the past, probably because each week is a mystery and a challenge, and that's pretty thrilling stuff. I still really really want to get better at the interactive aspects of the course, so that's my continuing goal for the coming week.

# Fail Log: Module 2 - 2017-07-23 #

### Exercise 1: The Dream Case ###
* I searched the Commonwealth War Graves Commission for examples of well-structured data and downloaded my results to analyze the .csv file.
* I decided to search for a few relatives I know died in the second world war, but was only able to find one. 
* The 'additional information' section of the .csv confirmed that this was my great uncle Arthur William Craig, *"Son of William A. Craig and Louisa A. Craig, of Ottawa; husband of Jean Craig, of Ottawa."* 
I recognized these family members from my Dad's extensive genelogy work - Pretty cool!
* Something to note: I found limiting the other search terms when I knew the full name of the person I was looking for was helpful. Oddly enough, the less specific I was, the more likely I was to find an interesting result.
* Notes from this exercise are in my github repository [here](https://github.com/catherinesupplecraig/HIST3814o/blob/master/m2e1.md) - the formatting on this file looks a bit weird in github. Not sure if that's something I messed up when I was writing it in Nano or not.

### Exercise 2: Wget ###
* I ran through Ian Milligan's [exercise](https://programminghistorian.org/lessons/automated-downloading-with-wget#step-two-learning-about-the-structure-of-wget--downloading-a-specific-set-of-files) for learning the ins and outs of wget, and successfully downloaded the http://activehistory.ca/papers/ directory!
* I used wget to grab a decade's worth of issues of the Shawville Equity, but hit a bit of a snag while doing so. I’m pretty sure the command I used was fine (I scrolled down to see the example), and everything looked okay on the screen - I didn’t get any error messages or anything, but the files never showed up the in directory that I created. 
* I decided to try a test with just one year and it worked totally fine so I ended up using the decade by decade method `wget http://collections.banq.qc.ca:8008/jrn03/equity/src/2001/ -A .txt -r --no-parent -nd –w 2 --limit-rate=20k` etc. 
* Later I went back and tried using the command again to grab a whole (different) decade, and it worked fine this time. I suspect that the reason I encounted a glitch the first time was because the decade I picked was 2000-2010. I knew I would have to go and grab 2010 using the command for one year anyway, but maybe 200* caused some kind of glitch? I may play around with this again and see what happens. 
*  `CTRL + C` is the command I never knew I needed…this would have been really helpful to know when I kept screwing up on the command line last week by using the arrow keys when I shouldn’t have!

### Exercise 3: TEI ###
* I began the process of transcribing and marking up a page from the abolistionist pamphlet "Negro Slavery" by Zachary Macaulay (found at recoveredhistories.org)
* I downloaded Sublime Text to do this
* I left this exercise last, since it was mentioned that we could choose to do it at a later time if necessary, and ended up cutting my attempt short after realizing that the page I had selected (and already transcribed) didn't have any names, specific locations, and only one claim...so productive fail and lesson learned for next time, I need to make sure to read through all the instructions *carefully* before starting the work!

### Exercise 4: APIs ###
* I added `&fmt=json` to the end of a search query at the Canadiana Discovery Portal to see the results in .json format
* I ran through the exercise using [Ian Milligan's Oochim # retrieval program](https://ianmilligan.ca/api-example-sh/) to pull the results from a Canadiana search for 'Ottawa'
* I lodged my work history for this exercise in my github repository [here](https://github.com/catherinesupplecraig/HIST3814o/blob/master/dhbox-July22.md)
* This exercise was the most straightforward for me. No hiccups!

### Exercise 5: Mining Twitter ###
* I created a twitter account (which I'll keep private) and installed twarc as an application on twitter and also on my DH Box.
* I was able to run the wget command to retreive tweets with the hashtag #idlenomore and #cdnpoli with no major issues
* I ultimately hit the same snag with json2csv that everyone else did. My programming-savvy boyfriend has been following along with some of the exercises for fun, and he actually tried the Mining Twitter exercise earlier in the week using his own program on MatLab, inspired by [this post](https://blogs.mathworks.com/loren/2014/06/04/analyzing-twitter-with-matlab/). We tried to work through sarahmcole's [suggestions](https://hist3814o.slack.com/archives/C5PEMDU3V/p1500733113372538) for fixing the glitch with json2csv, but couldn't figure out the first part of the fix, concerning the formatting of the arrays. After four failed attempts to write a program that would fix the issue with the arrays and number each "tweet" we decided to call it quits. Because we were using MatLab I don't have the program the we (okay, he) wrote, but I do have the [before](https://github.com/catherinesupplecraig/HIST3814o/blob/master/search.json) and [after](https://github.com/catherinesupplecraig/HIST3814o/blob/master/search_fixed_final.JSON) versions of my .json file in my repository for safekeeping. 
* I'm very interested in this kind of data analysis (as a communications student it's right up my alley) so I'll definitely be coming back to try the fix posted in our group #slack [here](https://hist3814o.slack.com/files/dr.graham/F6DEUCQUF/twarc_json_to_csv__exercise_5).

### Exercise 6: OCR with Tesseract ###
* I installed tesseract, imagemagick, and pdftk on my DH Box
* I hit a snag using wget to get the correct page from the equity. While I figured out the command without any problems (all those hours struggling with wget earlier paid off, apparently) it was taking so long to dowload that DH Box kept timing out and asking me to log in, which caused me to have to start all over again. Eventually I figured out that for one page a rate of 20kb was probably lower than necessary (hopefully?) and I upped it quite a bit to make sure my command was going through quickly enough.
* The second snag came when it was time to burst the file. I kept getting a ‘file not found’ error, and eventually realized (by running ls) that my wget command had given me a series of folders. including these folders in the command to burst is one solution, but I ended up just using the file manager to move the pdf to my orc-test folder, and everything worked perfectly from there. Not sure if there is room for improvement in my wget command 
`wget -r --no-parent -w 2 --limit-rate=50khttp://collections.banq.qc.ca:8008/jrn03/equity/src/1957/07/04/83471_1957-07-04.pdf` or if this was an inevitable glitch?
* The output I got from this exercise was hilariously bad, and it's in my repository [here](https://github.com/catherinesupplecraig/HIST3814o/blob/master/output.txt) for reference. 

### Personal Fail ###
I included this personal fail section on a whim last week, but I've decided to keep it, because I think documenting areas where human failure occurs in the process of doing a project is actually pretty helpful. 
* I actually made a lot of progress on my procrastination this week - I worked on the exercises or readings every day, and even though I'll still be submitting everything on Sunday night, I got a lot done during the week. Something I'm starting to realize is that you could basically spend an infinite amount of time on this stuff, and there comes a certain point where cutting your losses and moving on is probably for the best. 
* An area where I struggled this week was with the interactive side of the course. I work in a job that requires a lot of collaboration, which is something I enjoy, and I'm usually among the first people in a class to raise my hand with a comment or question. Yet, for some reason, I'm finding this isn't translating very well in an online forum. Maybe it's because when it comes to writing I tend to approach things more in a more inward-facing way. Writing is my processing tool for dealing with pretty much any personal problem I encounter, so it always feels a bit personal for me. I wanted to make more of an effort to engage this week, since that's a piece of feedback I've received consistently for the past two weeks, but while I enjoy the functionality of hypothesis as a tool for marking up readings, the interactive part hasn't jivved with me too much. I'd like to improve this. I decided to leave my annotations for last this week so that I could let me thoughts about the readings marinate a little bit. I'm hoping that this will help me engage a little bit more. 
* I definitely get the 'digital history can be fun' side of things - sitting on the couch and running through these exercises, especially when my boyfriend is following along and hitting the same roadblocks, is really enjoyable. I like chatting with him about what I'm doing, it's just a bit more difficult to feel inclined to get social in #slack. I could go on with my self-psychoanalysis about why I think this is, but ultimately all it comes down to is that I want and *need* to get better at this side of the course. Goal for next week!
# Fail Log: Module 1, 2017-07-16 #

### Hypothesis ###
A fail from last week that I figured I'd make note of - hypothesis bugged out after I navigated to a new page in a different tab, and the comment I was working on got weirdly frozen so that I couldn't edit it or even post it. I tried to copy the text and refresh the page, but that failed too, so I ended up just rewriting. First (and so far only) hiccup that I've had using this tool. I've also found that I do a better job of grasping the readings if I leave the annotations turned off for the first time I look at an article, and only then go back and read with the annotations turned on. This is hard for me, beacause it's difficult to resist what other people have to say, but I find I get easily sidetracked from what I'm trying to absorb if I'm dividing my attention between too many different things.

### DH Box ###
It took me a (long) while to get the hang of working from the command line and I'm still only modestly sure I'm doing the right thing all the time. I mentioned how alien all this feels to a friend who pointed out that programming "languages" are really just that, and that learning them is akin to learning to read and write in a totally new way.

I had an issue with being able to connect to DH box on July 16 after it had been working perfectly earlier in the day. Eventually I realized that the VPN was malfunctioning, rather than DH Box itself. Even when I reconnected it took a few attempts for the VPN to stabilize enough for me to continue with exercise 4.

Another noteable thing for me - I need to remember that following instructions sometimes involves backtracking when you have to troubleshoot or set things up at the same time. There were a few times when I'd follow the instructions up until a point where we needed to install something, and then would try to move on to the next step, only to remember that I needed to complete the earlier step that I'd had to sidetrack from originally (if that makes sense?) Long stroy short, there's a lot to keep track of and sometimes I get ahead of myself.

I had a bunch of productive fails while going through the first part of exercise 2 (trying to save a file as html etc) because I'm so unused to the command line that I accidently changed something by hitting the < and > arrows somewhere I shouldn't have. I honeslty don't know how I managed to do so much damange by doing this, but the header to my text file completely disappeared. I also didn't read the instructions closely enough the first time and didn't know to make the date the header, and ended up having to redo the exercise a few times. This was actually pretty helpful though, because I ended up getting super comofortable going through the motions of opening the text editor and saving as different file types.

### Personal Fail ###
I'm in an ongoing battle with procrastination, and I'm really trying to work at being a better time manager this summer because I know that how miserable the coming year of school is going to be for me will ultimately be dictated by my ability to meet deadlines and stay on top of the goals I set for my work. That having been said, the past two weeks have been TOUGH. My grandmother passed away on Canada Day and I started working full time at my job a few days later without much time to process. Between these two things my motivation to stay on top of school stuff has been pretty zapped - definitely not the way I was hoping to start the summer. I guess the bright side is that even if I've been slow to do things, I have been getting them done. So I guess there's something to be said about the fact that if I can do it under these circumstances, I should be okay when things in my personal life are better. I also find that I'm less likely to procrastinate when I feel accountable to someone, so doing this course in a public forum is helpful in the sense that if I drop the ball, it's out there for everyone to see. Makes me wonder if open research might be useful for me next year as I work on my honours thesis.
